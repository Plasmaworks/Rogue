$include "VM.rogue"

class Tokenizer
  PROPERTIES
    filepath    : String
    reader      : Scanner
    tokens      = Token[]
    buffer      = StringBuilder()

    next_filepath : String
    next_line     : Int32
    next_column   : Int32

  METHODS
    method tokenize( filepath )->Token[]
      return tokenize( Scanner(File(filepath),2) )

    method tokenize( filepath, content:String, line=null:Int32?, column=null:Int32? )->Token[]
      local scanner = Scanner( content, 2 )
      if (line.exists)   scanner.line = line.value
      if (column.exists) scanner.column = column.value
      return tokenize( scanner )

    method tokenize( reference_t:Token, filepath, data:String, column_delta=0:Int32 )->Token[]
      local characters = Character[]( data.count )
      forEach (ch in data) characters.add( ch )
      return tokenize( Scanner(characters,2).[set_location(reference_t.line,reference_t.column+column_delta)] )

    method tokenize( reader )->Token[]
      RogueC.scanners_by_filepath[ filepath ] = reader

      configure_token_types
      while (tokenize_another) noAction

      if (tokens.count == 0)
        # Ensure there's an EOL token at the end.
        if (tokens.count == 0 or tokens.last.type is not TokenType.eol)
          add_new_token( TokenType.eol )
        endIf
      endIf

      return tokens

    # -------------------------------------------------------------------------

    method add_new_string_or_character_token_from_buffer( terminator:Character )->Logical
      # We have a string in 'buffer' already; convert it to a character if it has count 1.
      if (buffer.count == 1 and terminator == '\'')
        return add_new_token( TokenType.literal_character, buffer[0]->Character )
      else
        return add_new_token( TokenType.literal_string, buffer->String )
      endIf

    method add_new_token( type:TokenType )->Logical
      tokens.add( type.create_token(next_filepath,next_line,next_column) )
      return true

    method add_new_token( type:TokenType, value:Character )->Logical
      tokens.add( type.create_token(next_filepath,next_line,next_column,value) )
      return true

    method add_new_token( type:TokenType, value:Int64 )->Logical
      tokens.add( type.create_token(next_filepath,next_line,next_column,value) )
      return true

    method add_new_token( type:TokenType, value:Int32 )->Logical
      tokens.add( type.create_token(next_filepath,next_line,next_column,value) )
      return true

    method add_new_token( type:TokenType, value:Real64 )->Logical
      tokens.add( type.create_token(next_filepath,next_line,next_column,value) )
      return true

    method add_new_token( type:TokenType, value:String )->Logical
      tokens.add( type.create_token(next_filepath,next_line,next_column,value) )
      return true

    method configure_token_types
      if (TokenType.lookup) return

      TokenType.lookup        = Table<<String,TokenType>>()

      # Directives
      TokenType.meta_auto_id         = define( DirectiveTokenType("$auto_id") )
      TokenType.meta_block           = define( DirectiveTokenType("$block") )
      TokenType.meta_endBlock        = define( DirectiveTokenType("$endBlock") )
      TokenType.meta_define          = define( DirectiveTokenType("$define") )
      TokenType.meta_defined         = define( DirectiveTokenType("defined") )
      TokenType.meta_error           = define( DirectiveTokenType("$error") )
      TokenType.meta_essential       = define( DirectiveTokenType("$essential") )
      TokenType.meta_if              = define( DirectiveTokenType("$if") )
      TokenType.meta_elseIf          = define( DirectiveTokenType("$elseIf") )
      TokenType.meta_else            = define( DirectiveTokenType("$else") )
      TokenType.meta_endIf           = define( DirectiveTokenType("$endIf") )
      TokenType.meta_include         = define( DirectiveTokenType("$include") )
      TokenType.meta_includeFolder   = define( DirectiveTokenType("$includeFolder") )
      TokenType.meta_join            = define( DirectiveTokenType("$join") )
      TokenType.meta_localDefine     = define( DirectiveTokenType("$localDefine") )
      TokenType.meta_localMacro      = define( DirectiveTokenType("$localMacro") )
      TokenType.meta_macro           = define( DirectiveTokenType("$macro") )
      TokenType.meta_endMacro        = define( DirectiveTokenType("$endMacro") )
      TokenType.meta_endMetacode     = define( DirectiveTokenType("$endMetacode") )
      TokenType.meta_methodSignature = define( DirectiveTokenType("$methodSignature") )
      TokenType.meta_module          = define( StructuralDirectiveTokenType("module") )
      TokenType.meta_moduleName      = define( DirectiveTokenType("$moduleName") )
      TokenType.meta_requireRogue    = define( DirectiveTokenType("$requireRogue") )
      TokenType.meta_rogueVersion    = define( DirectiveTokenType("$rogueVersion") )
      TokenType.meta_string          = define( DirectiveTokenType("$string") )
      TokenType.meta_uses            = define( StructuralDirectiveTokenType("uses") )
      TokenType.meta_warning         = define( DirectiveTokenType("$warning") )

      # The following exist but are immediately converted by the tokenizer
      # $sourceFilepath
      # $sourceLine

      TokenType.placeholder_id         = define( TokenType("$id") )

      # Structure Tokens
      TokenType.eol                    = define( EOLTokenType("end of line").mark_end_command )
      TokenType.eoi                    = define( StructureTokenType("end of input").mark_non_method )
      TokenType.begin_augment_tokens   = define( StructureTokenType("(beginning of augment)").mark_non_method )
      TokenType.keyword_augment        = define( StructureTokenType("augment").mark_non_method )
      TokenType.keyword_case           = define( StructureTokenType("case") )
      TokenType.keyword_caseNext       = define( StructureTokenType("caseNext") )
      TokenType.keyword_catch          = define( StructureTokenType("catch") )
      TokenType.keyword_class          = define( StructureTokenType("class").mark_non_method )
      TokenType.keyword_CATEGORIES     = define( StructureTokenType("CATEGORIES").mark_non_method )
      TokenType.keyword_compileArg     = define( StructureTokenType("compileArg") )
      TokenType.keyword_DEFINITIONS    = define( StructureTokenType("DEFINITIONS").mark_non_method )
      TokenType.keyword_DEPENDENCIES   = define( StructureTokenType("DEPENDENCIES").mark_non_method )
      TokenType.keyword_else           = define( StructureTokenType("else") )
      TokenType.keyword_elseIf         = define( StructureTokenType("elseIf") )
      TokenType.keyword_endAugment     = define( StructureTokenType("endAugment").mark_non_method )
      TokenType.keyword_endBlock       = define( StructureTokenType("endBlock") )
      TokenType.keyword_endClass       = define( StructureTokenType("endClass").mark_non_method )
      TokenType.keyword_endContingent  = define( StructureTokenType("endContingent") )
      TokenType.keyword_endEnum        = define( StructureTokenType("endEnum") )
      TokenType.keyword_endForEach     = define( StructureTokenType("endForEach") )
      TokenType.keyword_endFunction    = define( StructureTokenType("endFunction") )
      TokenType.keyword_endIf          = define( StructureTokenType("endIf") )
      TokenType.keyword_endLoop        = define( StructureTokenType("endLoop") )
      TokenType.keyword_endRoutine     = define( StructureTokenType("endRoutine").mark_non_method )
      TokenType.keyword_endSubclass    = define( StructureTokenType("endSubclass") )
      TokenType.keyword_endTemporarily = define( StructureTokenType("endTemporarily") )
      TokenType.keyword_endTry         = define( StructureTokenType("endTry") )
      TokenType.keyword_endUnitTest    = define( StructureTokenType("endUnitTest") )
      TokenType.keyword_endUse         = define( StructureTokenType("endUse") )
      TokenType.keyword_endWhich       = define( StructureTokenType("endWhich") )
      TokenType.keyword_endWhile       = define( StructureTokenType("endWhile") )
      TokenType.keyword_ENUMERATE      = define( StructureTokenType("ENUMERATE").mark_non_method )
      TokenType.keyword_enum           = define( StructureTokenType("enum").mark_non_method )
      TokenType.keyword_global         = define( StructureTokenType("global").mark_non_method )
      TokenType.keyword_GLOBAL         = define( StructureTokenType("GLOBAL").mark_non_method )
      TokenType.keyword_method         = define( StructureTokenType("method").mark_non_method )  # new method ends the old one
      TokenType.keyword_METHODS        = define( StructureTokenType("METHODS").mark_non_method )
      TokenType.keyword_others         = define( StructureTokenType("others") )
      TokenType.keyword_PROPERTIES     = define( StructureTokenType("PROPERTIES").mark_non_method )
      TokenType.keyword_routine        = define( StructureTokenType("routine").mark_non_method )
      TokenType.keyword_satisfied      = define( StructureTokenType("satisfied") )
      TokenType.keyword_subclass       = define( StructureTokenType("subclass") )
      TokenType.keyword_unsatisfied    = define( StructureTokenType("unsatisfied") )
      TokenType.keyword_with           = define( StructureTokenType("with") )

      TokenType.symbol_close_brace           = define( StructureTokenType("}") )
      TokenType.symbol_close_bracket         = define( StructureTokenType("]") )
      TokenType.symbol_close_comment         = define( StructureTokenType("}#") )
      TokenType.symbol_close_paren           = define( StructureTokenType(")") )
      TokenType.symbol_close_specialize      = define( StructureTokenType(">>") )
      TokenType.symbol_comma                 = define( StructureTokenType(",") )

      # Statement Tokens
      TokenType.keyword_assert              = define( TokenType("assert") )
      TokenType.keyword_await               = define( TokenType("await") )
      TokenType.keyword_block               = define( TokenType("block") )
      TokenType.keyword_compileError        = define( TokenType("compileError") )
      TokenType.keyword_contingent          = define( TokenType("contingent") )
      TokenType.keyword_deprecated          = define( TokenType("deprecated") )
      TokenType.keyword_ensure              = define( TokenType("ensure") )
      TokenType.keyword_escapeBlock         = define( TokenType("escapeBlock") )
      TokenType.keyword_escapeContingent    = define( TokenType("escapeContingent") )
      TokenType.keyword_escapeForEach       = define( TokenType("escapeForEach") )
      TokenType.keyword_escapeIf            = define( TokenType("escapeIf") )
      TokenType.keyword_escapeLoop          = define( TokenType("escapeLoop") )
      TokenType.keyword_escapeTemporarily   = define( TokenType("escapeTemporarily") )
      TokenType.keyword_escapeTry           = define( TokenType("escapeTry") )
      TokenType.keyword_escapeUse           = define( TokenType("escapeUse") )
      TokenType.keyword_escapeWhich         = define( TokenType("escapeWhich") )
      TokenType.keyword_escapeWhile         = define( TokenType("escapeWhile") )
      TokenType.keyword_forEach             = define( TokenType("forEach") )
      TokenType.keyword_function            = define( TokenType("function") )
      TokenType.keyword_if                  = define( TokenType("if") )
      TokenType.keyword_in                  = define( TokenType("in") )
      TokenType.keyword_includeNativeCode   = define( TokenType("includeNativeCode") )
      TokenType.keyword_includeNativeHeader = define( TokenType("includeNativeHeader") )
      TokenType.keyword_includeSource       = define( TokenType("includeSource") )
      TokenType.keyword_includeSourceFolder = define( TokenType("includeSourceFolder") )
      TokenType.keyword_is                  = define( TokenType("is") )
      TokenType.keyword_isAspect            = define( TokenType("isAspect") )
      TokenType.keyword_isClass             = define( TokenType("isClass") )
      TokenType.keyword_isComparable        = define( TokenType("isComparable") )
      TokenType.keyword_isCompound          = define( TokenType("isCompound") )
      TokenType.keyword_isPrimitive         = define( TokenType("isPrimitive") )
      TokenType.keyword_isReference         = define( TokenType("isReference") )
      TokenType.keyword_isString            = define( TokenType("isString") )
      TokenType.keyword_isType              = define( TokenType("isType") )
      TokenType.keyword_local               = define( TokenType("local") )
      TokenType.keyword_localize            = define( TokenType("localize") )
      TokenType.keyword_loop                = define( TokenType("loop") )
      TokenType.keyword_native              = define( TokenType("native") )
      TokenType.keyword_nativeCode          = define( NativeCodeTokenType("nativeCode") )
      TokenType.keyword_nativeHeader        = define( NativeCodeTokenType("nativeHeader") )
      TokenType.keyword_nativeLibrary       = define( TokenType("nativeLibrary") )
      TokenType.keyword_necessary           = define( TokenType("necessary") )
      TokenType.keyword_noAction            = define( TokenType("noAction") )
      TokenType.keyword_null                = define( TokenType("null") )
      TokenType.keyword_of                  = define( TokenType("of") )
      TokenType.keyword_require             = define( TokenType("require") )
      TokenType.keyword_return              = define( TokenType("return") )
      TokenType.keyword_nextIteration       = define( TokenType("nextIteration") )
      TokenType.keyword_step                = define( TokenType("step") )
      TokenType.keyword_sufficient          = define( TokenType("sufficient") )
      TokenType.keyword_swapValues          = define( TokenType("swapValues") )
      TokenType.keyword_temporarily         = define( TokenType("temporarily") )
      TokenType.keyword_throw               = define( TokenType("throw") )
      TokenType.keyword_trace               = define( TokenType("trace") )
      TokenType.keyword_trace_args_only     = define( TokenType("@trace") )
      TokenType.keyword_try                 = define( TokenType("try") )
      TokenType.keyword_unitTest            = define( TokenType("unitTest") )
      TokenType.keyword_use                 = define( TokenType("use") )
      TokenType.keyword_which               = define( TokenType("which") )
      TokenType.keyword_while               = define( TokenType("while") )
      TokenType.keyword_yield               = define( TokenType("yield") )

      # Expression Tokens
      TokenType.identifier                   = TokenType("identifier")
      TokenType.type_identifier              = TokenType("type identifier")
      TokenType.literal_character            = TokenType("Character")
      TokenType.literal_int32                = TokenType("Int32")
      TokenType.literal_int64                = TokenType("Int64")
      TokenType.literal_real64               = TokenType("Real64")
      TokenType.literal_string               = TokenType("String")

      TokenType.keyword_and                  = define( TokenType("and") )
      TokenType.keyword_as                   = define( TokenType("as") )
      TokenType.keyword_downTo               = define( TokenType("downTo") )
      TokenType.keyword_false                = define( TokenType("false") )
      TokenType.keyword_instanceOf           = define( TokenType("instanceOf") )
      TokenType.keyword_infinity             = define( TokenType("infinity") )
      TokenType.keyword_meta                 = define( TokenType("meta") )
      TokenType.keyword_NaN                  = define( TokenType("NaN") )
      TokenType.keyword_not                  = define( TokenType("not") )
      TokenType.keyword_or                   = define( TokenType("or") )
      TokenType.keyword_pi                   = define( TokenType("pi") )
      TokenType.keyword_prior                = define( TokenType("prior") )
      TokenType.keyword_this                 = define( TokenType("this") )
      TokenType.keyword_ThisType             = define( TokenType("ThisType") )
      TokenType.keyword_true                 = define( TokenType("true") )
      TokenType.keyword_xor                  = define( TokenType("xor") )

      TokenType.symbol_ampersand             = define( TokenType("&") )
      TokenType.symbol_ampersand_equals      = define( ModifyAndAssignTokenType("&=") )
      TokenType.symbol_double_ampersand      = define( TokenType("&&") )
      TokenType.symbol_arrow                 = define( TokenType("->") )
      TokenType.symbol_at                    = define( TokenType("@") )
      TokenType.symbol_at_brace              = define( TokenType("@{") )
      TokenType.symbol_at_bracket            = define( TokenType("@[") )
      TokenType.symbol_backslash             = define( TokenType("\\") )
      TokenType.symbol_caret                 = define( TokenType("^") )
      TokenType.symbol_caret_equals          = define( ModifyAndAssignTokenType("^=") )
      TokenType.symbol_colon                 = define( TokenType(":") )
      TokenType.symbol_colon_colon           = define( TokenType("::") )
      TokenType.symbol_compare               = define( TokenType("<>") )
      TokenType.symbol_dollar                = define( TokenType("$") )
      TokenType.symbol_dot                   = define( TokenType(".") )
      TokenType.symbol_dot_equals            = define( ModifyAndAssignTokenType(".=") )
      TokenType.symbol_downToGreaterThan     = define( TokenType("..>") )
      TokenType.symbol_empty_braces          = define( TokenType("{}") )
      TokenType.symbol_empty_brackets        = define( TokenType("[]") )
      TokenType.symbol_eq                    = define( TokenType("==") )
      TokenType.symbol_equals                = define( TokenType("=") )
      TokenType.symbol_exclamation_point     = define( TokenType("!") )
      TokenType.symbol_fat_arrow             = define( TokenType("=>") )
      TokenType.symbol_ge                    = define( TokenType(">=") )
      TokenType.symbol_gt                    = define( TokenType(">") )
      TokenType.symbol_le                    = define( TokenType("<=") )
      TokenType.symbol_lt                    = define( TokenType("<") )
      TokenType.symbol_minus                 = define( TokenType("-") )
      TokenType.symbol_minus_equals          = define( ModifyAndAssignTokenType("-=") )
      TokenType.symbol_minus_minus           = define( TokenType("--") )
      TokenType.symbol_ne                    = define( TokenType("!=") )
      TokenType.symbol_open_brace            = define( TokenType("{") )
      TokenType.symbol_open_bracket          = define( TokenType("[") )
      TokenType.symbol_open_paren            = define( TokenType("(") )
      TokenType.symbol_open_specialize       = define( TokenType("<<") )
      TokenType.symbol_percent               = define( TokenType("%") )
      TokenType.symbol_percent_equals        = define( ModifyAndAssignTokenType("%=") )
      TokenType.symbol_plus                  = define( TokenType("+") )
      TokenType.symbol_plus_equals           = define( ModifyAndAssignTokenType("+=") )
      TokenType.symbol_plus_plus             = define( TokenType("++") )
      TokenType.symbol_question_mark         = define( TokenType("?") )
      TokenType.symbol_select                = define( TokenType("?:") )
      TokenType.symbol_semicolon             = define( TokenType(";").mark_end_command )
      TokenType.symbol_shift_left            = define( TokenType(":<<:") )
      TokenType.symbol_shift_right           = define( TokenType(":>>:") )
      TokenType.symbol_shift_right_x         = define( TokenType(":>>>:") )
      TokenType.symbol_slash                 = define( TokenType("/") )
      TokenType.symbol_slash_equals          = define( ModifyAndAssignTokenType("/=") )
      TokenType.symbol_slash_slash           = define( TokenType("//") )
      TokenType.symbol_tilde                 = define( TokenType("~") )
      TokenType.symbol_tilde_equals          = define( ModifyAndAssignTokenType("~=") )
      TokenType.symbol_times                 = define( TokenType("*") )
      TokenType.symbol_times_equals          = define( ModifyAndAssignTokenType("*=") )
      TokenType.symbol_upTo                  = define( TokenType("..") )
      TokenType.symbol_upToLessThan          = define( TokenType("..<") )
      TokenType.symbol_vertical_bar          = define( TokenType("|") )
      TokenType.symbol_vertical_bar_equals   = define( ModifyAndAssignTokenType("|=") )
      TokenType.symbol_double_vertical_bar   = define( TokenType("||") )

      # Noise words
      TokenType.keyword_do                   = define( TokenType("do") )

    method consume( ch:Character )->Logical
      if (reader.peek != ch) return false
      reader.read
      return true

    method consume( st:String )->Logical
      return reader.consume(st)

    method consume_id( st:String )->Logical
      return reader.consume_id(st)

    method consume_spaces->Logical
      if (not reader.consume(' ')) return false
      while (reader.consume(' '))  noAction
      return true

    method define( type:TokenType )->TokenType
      TokenType.lookup[type.name] = type
      return type

    method error( message:String )->RogueError
      return RogueError( message, filepath, reader.line, reader.column )

    method get_symbol_token_type->TokenType
      local ch = reader.read

      if (ch == '!')
        if (consume('=')) return TokenType.symbol_ne
        else              return TokenType.symbol_exclamation_point

      elseIf (ch == '$' )
        ch = reader.peek
        if (consume('{'))
          handle_metacode( &uses_shorthand )
          return null
        endIf
        if (not (ch.is_letter or ch == '_')) return TokenType.symbol_dollar

        local id = read_identifier
        which (id)
          case "auto_id"
            add_new_token( TokenType.identifier, Program.create_unique_id )
            return null

          case "define"
            add_new_token( TokenType.meta_define )

            local old_pos = reader.position
            consume_spaces
            local defined_id = read_identifier
            if (reader.peek != ' ')
              local mesg = "A space is required between '$define $' and '$'." ('$',defined_id,reader.peek)
              if (reader.peek == '(') mesg += "\nWrite '$macro $(arg1,arg2,...)' instead if you want to define a macro." ('$',defined_id)
              throw error( mesg )
            endIf
            reader.seek( old_pos )

          case "defined"
            consume_spaces
            local has_parens = consume( '(' )
            consume_spaces
            add_new_token( TokenType.meta_defined, read_identifier )
            consume_spaces
            if (has_parens and not consume(')')) throw error( "Closing ')' expected." )
            return null

          case "error":               add_new_token( TokenType.meta_error )
          case "include":             add_new_token( TokenType.meta_include )
          case "includeFolder":       add_new_token( TokenType.meta_includeFolder )
          case "if":                  add_new_token( TokenType.meta_if )
          case "elseIf":              add_new_token( TokenType.meta_elseIf )
          case "else":                add_new_token( TokenType.meta_else )
          case "endIf":               add_new_token( TokenType.meta_endIf )
          case "macro":               add_new_token( TokenType.meta_macro )
          case "metacode":            handle_metacode; return null
          case "endMacro":            add_new_token( TokenType.meta_endMacro )
          case "endMetacode":         add_new_token( TokenType.meta_endMetacode )
          case "warning":             add_new_token( TokenType.meta_warning )
          case "essential":           scan_essential_directive; return null
          case "sourceFilepath":      add_new_token( TokenType.literal_string, next_filepath )
          case "sourceLine":          handle_source_line_directive
          case "block":               add_new_token( TokenType.meta_block )
          case "endBlock":            add_new_token( TokenType.meta_endBlock )
          case "join":                add_new_token( TokenType.meta_join )
          case "localDefine":         add_new_token( TokenType.meta_localDefine )
          case "localMacro":          add_new_token( TokenType.meta_localMacro )
          case "methodSignature":     add_new_token( TokenType.meta_methodSignature )
          case "moduleName":          add_new_token( TokenType.meta_moduleName )
          case "requireRogue":        add_new_token( TokenType.meta_requireRogue )
          case "rogueVersion":        add_new_token( TokenType.meta_rogueVersion )
          case "string":              add_new_token( TokenType.meta_string )
          others
            add_new_token( TokenType.placeholder_id, "$" + id )
        endWhich
        return null

      elseIf (ch == '%')
        if     (consume('=')) return TokenType.symbol_percent_equals
        else                  return TokenType.symbol_percent


      elseIf (ch == '&' )
        if     (consume('&')) return TokenType.symbol_double_ampersand
        elseIf (consume('=')) return TokenType.symbol_ampersand_equals
        else                  return TokenType.symbol_ampersand

      elseIf (ch == '(')
        return TokenType.symbol_open_paren

      elseIf (ch == ')')
      return TokenType.symbol_close_paren

      elseIf (ch == '*')
        if     (consume('=')) return TokenType.symbol_times_equals
        else                  return TokenType.symbol_times

      elseIf (ch == '+')
        if     (consume('=')) return TokenType.symbol_plus_equals
        elseIf (consume('+')) return TokenType.symbol_plus_plus
        else                  return TokenType.symbol_plus

      elseIf (ch == ',')
        return TokenType.symbol_comma

      elseIf (ch == '-')
        if     (consume('='))  return TokenType.symbol_minus_equals
        elseIf (consume('-'))  return TokenType.symbol_minus_minus
        elseIf (reader.peek(0) == '>' and reader.peek(1) == '>') return TokenType.symbol_minus
        elseIf (consume('>'))  return TokenType.symbol_arrow
        else                   return TokenType.symbol_minus

      elseIf (ch == '.' )
        if (consume('.'))
          if (consume('.'))
            # ellipsis (line continuation modifier)
            # - If occurs at EOL, removes EOL - just consume EOL and continue parsing without creating a token
            # - If occurs mid-line, adds extra EOL
            while (consume(' ')) noAction
            if (not consume('\n')) add_new_token( TokenType.eol )
            return null
          elseIf (consume('<'))
            return TokenType.symbol_upToLessThan
          elseIf (consume('>'))
            return TokenType.symbol_downToGreaterThan
          else
            return TokenType.symbol_upTo
          endIf
        elseIf (consume('='))
          return TokenType.symbol_dot_equals
        else
          return TokenType.symbol_dot
        endIf

      elseIf (ch == '/')
        if (consume('/'))
          return TokenType.symbol_slash_slash
        elseIf (consume('='))
          return TokenType.symbol_slash_equals
        else
          return TokenType.symbol_slash
        endIf

      elseIf (ch == ':')
        if (consume(':'))
          if (reader.peek.is_letter or reader.peek == '_')
            add_new_token( TokenType.identifier, "::" + read_identifier )
            return null
          else
            return TokenType.symbol_colon_colon
          endIf
        endIf

        if (consume("<<:"))      return TokenType.symbol_shift_left
        elseIf (consume(">>:"))  return TokenType.symbol_shift_right
        elseIf (consume(">>>:")) return TokenType.symbol_shift_right_x
        return TokenType.symbol_colon

      elseIf (ch == ';')
        return TokenType.symbol_semicolon

      elseIf (ch == '<' )
        if     (consume('<')) return TokenType.symbol_open_specialize
        elseIf (consume('=')) return TokenType.symbol_le
        elseIf (consume('>')) return TokenType.symbol_compare
        else                  return TokenType.symbol_lt

      elseIf (ch == '=' )
        if (consume('='))     return TokenType.symbol_eq
        elseIf (consume('>')) return TokenType.symbol_fat_arrow
        else                  return TokenType.symbol_equals

      elseIf (ch == '>' )
        if      (consume('=')) return TokenType.symbol_ge
        elseIf  (consume('>')) return TokenType.symbol_close_specialize
        else                   return TokenType.symbol_gt

      elseIf (ch == '?')
        if   (consume(':')) return TokenType.symbol_select
        else                return TokenType.symbol_question_mark

      elseIf (ch == '@' )
        if (consume_id("trace")) return TokenType.keyword_trace_args_only
        if (consume('{'))        return TokenType.symbol_at_brace
        if (consume('['))        return TokenType.symbol_at_bracket
        return TokenType.symbol_at

      elseIf (ch == '[')
        if (consume(']')) return TokenType.symbol_empty_brackets
        return TokenType.symbol_open_bracket

      elseIf (ch == '\\')
        return TokenType.symbol_backslash

      elseIf (ch == ']')
        return TokenType.symbol_close_bracket

      elseIf (ch == '^')
        if     (consume('=')) return TokenType.symbol_caret_equals
        else                  return TokenType.symbol_caret

      elseIf (ch == '{')
        if (consume('}')) return TokenType.symbol_empty_braces
        return TokenType.symbol_open_brace

      elseIf (ch == '|' )
        if     (consume('|')) return TokenType.symbol_double_vertical_bar
        elseIf (consume('=')) return TokenType.symbol_vertical_bar_equals
        else                  return TokenType.symbol_vertical_bar

      elseIf (ch == '}' )
        if     (consume('#')) return TokenType.symbol_close_comment # used for error reporting
        elseIf (consume('$')) return TokenType.meta_endMetacode
        else                  return TokenType.symbol_close_brace

      elseIf (ch == '~')
        if     (consume('=')) return TokenType.symbol_tilde_equals
        else                  return TokenType.symbol_tilde

      else
        throw error( "Unexpected input '"+ch+"'." )
      endIf

    method handle_source_line_directive
      if (consume('('))
        if (not consume(')'))
          local line = scan_long
          if (not consume(')')) throw error( "Closing ')' expected." )
          next_line = line
          reader.line = line
          return
        endIf
      endIf

      add_new_token( TokenType.literal_int32, next_line )

    method next_is_hex_digit->Logical
      local ch = reader.peek
      return (ch >= '0' and ch <= '9') or (ch >= 'a' and ch <= 'f') or (ch >= 'A' and ch <= 'F')

    method read_character
      if (not reader.has_another) throw error( "Character expected." )

      local ch = reader.peek
      if (ch == '\n') throw error( "Character expected; found end of line." )

      if (ch == '\\')
        reader.read
        if (not reader.has_another) throw error( "Escaped character expected; found end of input." )

        local value : Int32
        if     (consume('b')) value = 8
        elseIf (consume('e')) value = 27
        elseIf (consume('f')) value = 12
        elseIf (consume('n')) value = '\n'
        elseIf (consume('r')) value = '\r'
        elseIf (consume('t')) value = '\t'
        elseIf (consume('v')) value = 11
        elseIf (consume('0')) value = '\0'
        elseIf (consume('/')) value = '/'
        elseIf (consume('?')) value = '?'
        elseIf (consume('\''))value = '\''
        elseIf (consume('\\'))value = '\\'
        elseIf (consume('"')) value = '"'
        elseIf (consume('x')) value = read_hex_value(2)
        elseIf (consume('u')) value = read_hex_value(4)
        elseIf (consume('[')) read_hex_sequence; return
        else throw error( "Invalid escape sequence.  Supported: \\b \\e \\f \\n \\r \\t \\v \\0 \\? \\/ \\' \\\\ \\\" \\" + "xHH \\" + "uHHHH \\" + "[H*]." )

        buffer.print( value->Character )

      else
        buffer.print( reader.read )

      endIf

    method read_hex_value( digits=6:Int32, &variable_length )->Int32
      local value = 0
      local i = 1
      while (i <= digits)
        if (not reader.has_another) throw error( digits + "-digit hex value expected; found end of file." )
        if (not next_is_hex_digit)
          if (variable_length) return value
          local ch = reader.peek
          local error_buffer = StringBuilder()
          error_buffer.print( "Invalid hex digit " )
          if (ch < ' ' or ch->Int32 == 127) error_buffer.print( ch->Int32 )
          else error_buffer.print( "'" + ch + "'" )
          error_buffer.print('.')
          throw error( error_buffer->String )
        endIf
        local intval = reader.read.to_number(16)
        value = (value:<<:4) + intval
        ++i
      endWhile
      return value

    method read_hex_sequence
      local first = true
      while (first or consume(','))
        first = false
        local ch = read_hex_value( &variable_length )->Character
        buffer.print( ch )
      endWhile

      if (consume(']')) return
      throw error( "Closing ']' expected." )

    method read_identifier->String
      buffer.clear
      local ch = reader.peek
      while ((ch>='a' and ch<='z') or (ch>='A' and ch<='Z') or (ch>='0' and ch<='9') or ch=='_')
        buffer.print( reader.read )
        ch = reader.peek
        while (ch == ':' and reader.peek(1) == ':')
          reader.read
          reader.read
          buffer.print( "::" )
          ch = reader.peek
        endWhile
      endWhile

      if (buffer.count == 0) throw error( "Identifier expected." )

      return buffer->String

    method tokenize_alternate_string( terminator:Character )->Logical
      buffer.clear

      while (reader.has_another)
        if (reader.has_another)
          local ch = reader.peek
          if (ch == terminator)
            reader.read
            ch = reader.peek
            if (ch == terminator)
              reader.read
              return add_new_string_or_character_token_from_buffer(0)
            else
              buffer.print( terminator )
            endIf
          else
            read_character
          endIf
        endIf
      endWhile

      throw error( "End of file reached while looking for end of string." )

    method tokenize_another->Logical
      reader.consume_spaces

      # Must be before the has_another test for the terminating EOL
      next_filepath = filepath
      next_line = reader.line
      next_column = reader.column

      if (not reader.has_another) return false

      local ch = reader.peek
      if (ch == '\n') reader.read; return add_new_token( TokenType.eol )

      if (ch.is_letter or ch == '_')
        local id = read_identifier
        local keyword_type = TokenType.lookup[id]
        if (keyword_type)
          if (keyword_type is TokenType.keyword_nativeCode)
            return scan_native_code
          elseIf (keyword_type is TokenType.keyword_nativeHeader)
            return scan_native_header
          elseIf (keyword_type is TokenType.meta_essential)
            return scan_essential_directive
          elseIf (keyword_type is TokenType.meta_uses)
            local has_placeholder_id = false
            local n = 0
            while (reader.has_another(n) and reader.peek(n) != '\n')
              if (reader.peek(n) == '$')
                has_placeholder_id = true
                escapeWhile
              endIf
              ++n
            endWhile
            if (has_placeholder_id)
              # uses A/$B
              #   ->
              # uses A
              # uses $B
              consume_spaces
              local first = true
              while (first or consume('/'))
                if (first) first = false
                else       add_new_token( TokenType.eol )
                add_new_token( TokenType.meta_uses )
                if (consume('$')) add_new_token( TokenType.placeholder_id, "$" + read_identifier )
                else              add_new_token( TokenType.identifier, read_identifier )
              endWhile
            else
              # uses A/B
              #   ->
              # uses "A/B"  # Not a string, just A/B combined into single identifier

              use module_id=StringBuilder.pool
                consume_spaces
                local first = true
                while (first or consume('/'))
                  if (first) first = false
                  else       module_id.print '/'
                  module_id.print( read_identifier )
                endWhile
                add_new_token( TokenType.meta_uses )
                add_new_token( TokenType.identifier, module_id->String )
              endUse
            endIf
            return true
          else
            return add_new_token( keyword_type )
          endIf
        else
          return add_new_token( TokenType.identifier, id )
        endIf
        return true

      elseIf (ch == '\'')
        if (reader.peek(1) == '\'')
          reader.read
          reader.read
          return tokenize_alternate_string( '\'' )
        else
          return tokenize_string( '\'' )
        endIf

      elseIf (ch == '"')
        return tokenize_string( '"' )

      elseIf (ch == '#')
        return tokenize_comment

      elseIf (ch == '0')
        which (reader.peek(1))
          case 'b': return tokenize_integer_in_base(2)
          case 'c': return tokenize_integer_in_base(8)
          case 'x': return tokenize_integer_in_base(16)
          others:   return tokenize_number
        endWhich

      elseIf (ch >= '0' and ch <= '9')
        return tokenize_number

      elseIf (ch == '@' and reader.peek(1) == '|')
        return tokenize_verbatim_string

      else
        # Either a symbol or a decimal like ".1".
        if (ch == '.')
          local next = reader.peek(1)
          if (next >= '0' and next <= '9') return tokenize_number
        endIf

        # Symbol
        local token_type = get_symbol_token_type
        if (token_type is null) return true
        return add_new_token( token_type )
      endIf

      local name = "'$'" (ch)
      if (ch == 10) name = "EOL"
      elseIf (ch < 32 or ch > 126) name = "(Unicode $)" (ch->Int32)

      throw error( "Syntax error: unexpected input $." (name) )

    method tokenize_comment->Logical
      buffer.clear
      reader.read  # '#'
      if (consume('{'))
        local nesting_count = 1
        while (reader.has_another)
          local ch = reader.read
          which (ch)
            case '#'
              buffer.print('#')
              if (consume('{'))
                buffer.print('{')
                ++nesting_count
              endIf

            case '}'
              if (consume('#'))
                --nesting_count
                if (nesting_count == 0) escapeWhile
                else buffer.print('}'); buffer.print('#')
              else
                buffer.print('}')
              endIf

            others
              buffer.print( ch )
          endWhich
        endWhile
      else
        while (reader.has_another and reader.peek != '\n') buffer.print( reader.read )
      endIf

      local comment = buffer->String
      local keyword : String
      contingent
        forEach (todo_keyword in RogueC.todo_keywords)
          keyword = todo_keyword
          sufficient (comment.contains(keyword))
        endForEach
        escapeContingent

      satisfied
        local original_position = reader.position
        local line = reader.line
        if (comment.contains('\n'))
          # reader.line is at end of comment. Adjust todo keyword start position
          line -= comment.count('\n')
          line += comment.before_first(keyword).count('\n')
        endIf
        reader.seek_location( line, 1 )
        print ''[$][$:$] '' (keyword,File.filename(filepath),line)
        local consecutive_spaces = 0
        while (reader.has_another)
          local ch = reader.read
          if (ch == ' ')
            ++consecutive_spaces
            if (consecutive_spaces == 1) print ch
          else
            consecutive_spaces = 0
            print ch
          endIf
          if (ch == '\n') escapeWhile
        endWhile
        reader.seek( original_position )
      endContingent

      if (tokens.count and tokens.last.type is TokenType.eol)
        # Store comments in preceding EOL, if any.
        (tokens.last as EOLToken).comment += comment
      endIf

      return true

    method tokenize_integer_in_base( base:Int32 )->Logical
      reader.read  # '0'
      reader.read  # [b,c,x] = [2,8,16]

      local count = 0
      local n = 0 : Int64
      local ch = reader.peek
      local digit = ch.to_number( base )
      while (reader.has_another and (digit != -1 or ch == '_'))
        if (ch != '_')
          if (digit >= base) throw error( "Digit out of range for base " + base + "." )
          ++count
          n = n * base + digit
        endIf
        reader.read
        ch = reader.peek
        digit = ch.to_number( base )
      endWhile

      if (count == 0) throw error( "One or more digits expected." )

      # Check for Int32 vs Int64
      if ((base==2 and count>32) or (base==8 and count>11) or (base==16 and count>8))
        return add_new_token( TokenType.literal_int64, n )
      else
        return add_new_token( TokenType.literal_int32, n )
      endIf

    method tokenize_number->Logical
      local is_negative = consume('-')

      local i = 0
      if (reader.has_another(i+1) and reader.peek(i).is_number())
        ++i
        while (reader.has_another(i+1) and (reader.peek(i).is_number() or reader.peek(i) == '_')) ++i
      endIf
      local ch = reader.peek(i)
      local is_real = ((ch == '.' and reader.peek(i+1).is_number) or ch == 'e' or ch == 'E')

      if (is_real)
        # We have a real number
        local n = scan_real
        ch = reader.peek

        if (ch == '.')
          ch = reader.peek(1)
          if (ch >= '0' and ch <= '9')
            reader.read
            local underscore_count = 0
            i = 0
            while (reader.has_another(i+1) and (reader.peek(i).is_number() or reader.peek(i) == '_'))
              if (reader.peek(i) == '_') ++underscore_count
              ++i
            endWhile
            local start_pos = reader.position
            local fraction = scan_real
            n += fraction / 10.0^((reader.position - start_pos) - underscore_count)
          elseIf (ch == '.')
            # Start of range
            if (is_negative) n = -n
            return add_new_token( TokenType.literal_int32, n->Int32 )
          elseIf ((ch >= 'a' and ch <= 'z') or (ch >= 'A' and ch <= 'Z') or ch == '_')
            # E.g. 5.hash_code is (5).hash_code, not 5.0hashcode
            return add_new_token( TokenType.literal_int32, n->Int32 )
          else
            if (is_negative) n = -n
            return add_new_token( TokenType.literal_real64, n )
          endIf
        endIf

        if (consume('E') or consume('e'))
          local negative_exponent = consume('-')
          if (not negative_exponent) consume('+')
          local power = scan_real
          if (negative_exponent) n /= 10.0^power
          else                   n *= 10.0^power
        endIf

        if (is_negative) n = -n;
        return add_new_token( TokenType.literal_real64, n )

      else
        # Int32 or Long
        local n = scan_long
        if (is_negative) n = -n;

        if (n == n->Int32)
          return add_new_token( TokenType.literal_int32, n->Int32 )
        else
          return add_new_token( TokenType.literal_int64, n )
        endIf
      endIf

    method scan_real->Real64
      local n = 0.0
      local allow_underscore = false
      local ch = reader.peek
      while ((ch >= '0' and ch <= '9') or (allow_underscore and ch == '_'))
        if (ch == '_')
          reader.read
        else
          local intval = reader.read->Int32 - '0'
          n = n * 10 + intval
        endIf
        ch = reader.peek
        allow_underscore = true
      endWhile
      return n

    method scan_long->Int64
      local n = 0 : Int64
      local allow_underscore = false
      local ch = reader.peek
      while ((ch >= '0' and ch <= '9') or (allow_underscore and ch == '_'))
        if (ch == '_')
          reader.read
        else
          local intval = reader.read->Int32 - '0'
          n = n * 10 + intval
        endIf
        ch = reader.peek
        allow_underscore = true
      endWhile
      return n

    method scan_native_code->Logical
      local buffer = StringBuilder()
      reader.consume_spaces
      if (reader.consume('\n'))
        # Multi-line native code ending with endNativeCode
        local found_end = false
        while (reader.has_another)
          if (reader.column == 1)
            local spaces = 0
            while (reader.consume(' ')) ++spaces
            if (reader.consume_id("endNativeCode"))
              found_end = true
              escapeWhile
            else
              forEach (1..spaces) buffer.print( ' ' )
            endIf
          endIf
          buffer.print( reader.read )
        endWhile
        if (not found_end) throw error( "'endNativeCode' expected before EOF." )
      else
        # Single line native code to EOL
        while (reader.has_another)
          if (reader.peek == '\n') escapeWhile
          buffer.print( reader.read )
        endWhile
      endIf

      return add_new_token( TokenType.keyword_nativeCode, buffer->String )

    method scan_native_header->Logical
      local buffer = StringBuilder()
      reader.consume_spaces
      if (reader.consume('\n'))
        # Multi-line native code ending with endNativeHeader
        local found_end = false
        while (reader.has_another)
          if (reader.column == 1)
            local spaces = 0
            while (reader.consume(' ')) ++spaces
            if (reader.consume_id("endNativeHeader"))
              found_end = true
              escapeWhile
            else
              forEach (1..spaces) buffer.print( ' ' )
            endIf
          endIf
          buffer.print( reader.read )
        endWhile
        if (not found_end) throw error( "'endNativeHeader' expected before EOF." )
      else
        # Single line native code to EOL
        while (reader.has_another)
          if (reader.peek == '\n') escapeWhile
          buffer.print( reader.read )
        endWhile
      endIf

      return add_new_token( TokenType.keyword_nativeHeader, buffer->String )

    method scan_essential_directive->Logical
      local buffer = StringBuilder()
      reader.consume_spaces

      # Single line native code to EOL
      while (reader.has_another)
        if (reader.consume('\n')) escapeWhile
        buffer.print( reader.read )
      endWhile

      return add_new_token( TokenType.meta_essential, buffer->String.trimmed )

    method tokenize_string( terminator:Character )->Logical
      buffer.clear
      reader.read
      while (reader.has_another)
        local ch = reader.peek
        if (ch == terminator)
          reader.read
          return add_new_string_or_character_token_from_buffer(terminator)
        else
          read_character
        endIf
      endWhile

      throw error( "End of input reached while looking for end of string." )

    method tokenize_verbatim_string->Logical
      buffer.clear
      reader.read
      reader.read
      while (reader.has_another)
        local ch = reader.read
        if (ch == 10)
          consume_spaces
          if (consume('|'))
            buffer.print( ch )
          else
            add_new_token( TokenType.literal_string, buffer->String )
            return add_new_token( TokenType.eol )
          endIf
        else
          buffer.print( ch )
        endIf
      endWhile

      throw error( "End of File reached while looking for end of verbatim string." )

    method handle_metacode( &uses_shorthand )
      # Already scanned '$metacode' or '${'
      local hook = "inline"
      local is_multi_line = true
      if (not uses_shorthand)
        if (consume('<'))
          use builder=StringBuilder.pool
            while (reader.has_another and reader.peek != '>')
              builder.print( reader.read )
            endWhile
            hook = builder->String
            if (not consume('>')) throw error( "Closing '>' expected." )
          endUse
        endIf

        while (consume(' ')) noAction
        is_multi_line = consume('\n')
      endIf

      local start_index = tokens.count
      if (is_multi_line)
        # Multi-line
        while (tokenize_another)
          if (tokens.last.type is TokenType.meta_endMetacode) escapeWhile
        endWhile

        if (tokens.last.type is not TokenType.meta_endMetacode)
          if (uses_shorthand)
            throw tokens.last.error( "End of input while looking for '}$'." )
          else
            throw tokens.last.error( "End of input while looking for '$endMetacode'." )
          endIf
        endIf

        tokens.remove_last

      else
        # Single line
        while (tokenize_another)
          if (tokens.last.type is TokenType.eol) escapeWhile
        endWhile
      endIf

      local metatokens = tokens.subset( start_index )
      tokens.discard_from( start_index )

      local original_position = reader.position  # Because parsing any 'trace' messes up the read position

      Parser.is_parsing_metacode = true
      local parser = Parser( metatokens, &preprocess )
      parser.this_type = Program.type_Metacode
      local statements = CmdStatementList()

      parser.parse_multi_line_statements( statements )
      reader.seek( original_position )
      Parser.is_parsing_metacode = false

      if (hook == "inline")
        local vm = VM()
        vm.execute( this, statements )
        if (vm.inject.count)
          local new_filepath = "$ - Meta:$" (filepath,reader.line)
          tokens.add( Tokenizer().tokenize(new_filepath, vm.inject->String) )
          vm.inject.clear
        endIf
      else
        (ensure Program.metacode_hooks[hook]).add( statements )
      endIf

endClass

